# Import necessary libraries
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_selection import SelectFromModel
from sklearn.decomposition import PCA
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from imblearn.over_sampling import SMOTE
from imblearn.pipeline import Pipeline as ImbPipeline
import matplotlib.pyplot as plt
import seaborn as sns

# Load dataset
data = pd.read_csv("your_dataset.csv")

# Separate features and target variable
X = data.drop("target_variable", axis=1)
y = data["target_variable"]

# Define numerical and categorical features
numerical_features = X.select_dtypes(include=[np.number]).columns.tolist()
categorical_features = X.select_dtypes(include=[np.object]).columns.tolist()

# Create preprocessing transformers
numerical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='mean')),
    ('scaler', StandardScaler())
])

categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])

# Combine transformers into a preprocessor
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numerical_transformer, numerical_features),
        ('cat', categorical_transformer, categorical_features)
    ])

# Split data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define the model
model = RandomForestClassifier(n_estimators=100, random_state=42)

# Feature selection using SelectFromModel
feature_selector = SelectFromModel(model)
X_train_selected = feature_selector.fit_transform(preprocessor.fit_transform(X_train), y_train)

# Principal Component Analysis (PCA) for dimensionality reduction
pca = PCA(n_components=0.95)
X_train_pca = pca.fit_transform(preprocessor.fit_transform(X_train))

# Combine feature selection and PCA into a preprocessor
combined_preprocessor = ColumnTransformer(
    transformers=[
        ('num', numerical_transformer, numerical_features),
        ('cat', categorical_transformer, categorical_features),
        ('feature_selection', feature_selector, X.columns),
        ('pca', pca, X.columns)
    ])

# Define SMOTE for handling imbalanced classes
smote = SMOTE(random_state=42)

# Create an imbalanced pipeline with SMOTE
imbalanced_pipeline = ImbPipeline(steps=[
    ('preprocessor', combined_preprocessor),
    ('smote', smote),
    ('model', model)
])

# Define hyperparameter grid for GridSearchCV
param_grid = {
    'model__n_estimators': [50, 100, 150],
    'model__max_depth': [None, 10, 20],
    'model__min_samples_split': [2, 5, 10]
}

# Perform GridSearchCV for hyperparameter tuning
grid_search = GridSearchCV(imbalanced_pipeline, param_grid, cv=5, scoring='accuracy')
grid_search.fit(X_train, y_train)

# Best hyperparameters
best_params = grid_search.best_params_

# Evaluate the model
y_pred = grid_search.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
classification_rep = classification_report(y_test, y_pred)
confusion_mat = confusion_matrix(y_test, y_pred)

# Print evaluation metrics and best hyperparameters
print(f"Best Hyperparameters: {best_params}")
print(f"Accuracy: {accuracy}")
print("Classification Report:\n", classification_rep)
print("Confusion Matrix:\n", confusion_mat)

# Visualize feature importance
feature_importance = pd.Series(grid_search.best_estimator_['model'].feature_importances_, index=X.columns)
sorted_feature_importance = feature_importance.sort_values(ascending=False)
plt.figure(figsize=(10, 6))
sns.barplot(x=sorted_feature_importance, y=sorted_feature_importance.index)
plt.title('Feature Importance')
plt.show()